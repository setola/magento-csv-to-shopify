version: "3"

# Global variables shared across all tasks to eliminate duplication
# These are available in all tasks and can be overridden by task-specific vars or CLI parameters
vars:
  # File and directory paths
  CSV_DIR: '{{ .CSV_DIR | default "./data" }}'
  CSV_FILE: 
    sh: find {{ .CSV_DIR }} -maxdepth 1 -type f -iname "export_catalog_product_*.csv" | head -1
  CSV_FILE_TEST: '{{ .CSV_FILE_TEST | default (print .CSV_DIR "/products_test.csv") }}'
  LOGS_DIR: '{{ .LOGS_DIR | default "./logs" }}'
  
  # Docker configuration
  NODE_IMAGE: '{{ .NODE_IMAGE | default "node:alpine" }}'
  DOCKER_LOGS_DIR: '{{ .DOCKER_LOGS_DIR | default "/app/logs" }}'
  
  # Common flags for CSV tasks
  OUTPUT_FLAG: '{{ if .OUTPUT }}--output {{ .OUTPUT }}{{ end }}'
  COUNT_ONLY_FLAG: '{{ if eq .COUNT_ONLY "true" }}--count-only{{ end }}'
  NO_HEADER_FLAG: '{{ if eq .NO_HEADER "true" }}--no-header{{ end }}'
  CASE_INSENSITIVE_FLAG: '{{ if eq .CASE_INSENSITIVE "true" }}--case-insensitive{{ end }}'
  COUNT_FLAG: '{{ if eq .COUNT "true" }}--count{{ end }}'
  SORT_FLAG: '{{ if eq .SORT "true" }}--sort{{ end }}'

tasks:

  csv:count-rows:
    cmds:
      - |
        docker run --rm \
          --name csv-row-counter \
          --workdir /app \
          --volume "$(pwd):/app" \
          --env-file .env \
          {{ .NODE_IMAGE }} \
          node utils/CSVRowCounter.js {{ .CSV_FILE }}

  csv:search:
    desc: "Search for a term in Magento CSV files and extract complete rows"
    summary: |
      Search for a specific term in Magento product CSV files.
      Properly handles multi-line CSV content (e.g., HTML descriptions).
      
      Usage: go-task csv:search TERM=BIX.A-REM-70S
      Usage: go-task csv:search TERM=DAA.100358
      Usage: go-task csv:search TERM=DAA.100358 CSV_FILE=./export_catalog_product_20251010_200348.csv
      Usage: go-task csv:search TERM=SKU123 > output.csv
    vars:
      TERM: '{{.TERM | default "BIX.A-REM-70S"}}'
    silent: true
    cmds:
      - head -n1 {{ .CSV_FILE }}
      - |
        python3 -c "import csv,sys,io; [csv.writer(sys.stdout).writerow(row) for file in sys.argv[1:] for row in csv.reader(open(file)) if '{{.TERM}}' in ','.join(row)]" {{ .CSV_FILE }} 2>/dev/null || true

  csv:search-sku:
    desc: "Search for exact SKU match in the SKU column only"
    summary: |
      Search for an exact SKU match in the SKU column of Magento CSV files.
      More precise than general search - only matches the SKU field.
      
      Usage: go-task csv:search-sku SKU=BIX.A-REM-70S
      Usage: go-task csv:search-sku SKU=DAA.100358
      Usage: go-task csv:search-sku SKU=SKU123 > output.csv
    vars:
      SKU: '{{.SKU | default "BIX.A-REM-70S"}}'
    silent: true
    cmds:
      - |
        python3 -c "import csv,sys,io; [csv.writer(sys.stdout).writerow(row) for file in sys.argv[1:] for row in csv.reader(open(file)) if len(row) > 0 and row[0] == '{{.SKU}}']" {{ .CSV_FILE }} 2>/dev/null || true

  csv:distinct:
    desc: "Extract distinct values from a CSV column with analysis options"
    summary: |
      Extract distinct values from a specified column in Magento CSV files.
      Supports counting, sorting, and output to files.
      
      Usage Examples:
      go-task csv:distinct COLUMN=product_type
      go-task csv:distinct COLUMN=categories COUNT=true SORT=true
      go-task csv:distinct COLUMN=sku OUTPUT=sku_list.txt
      go-task csv:distinct -- --list-columns
      go-task csv:distinct COLUMN=manufacturer CSV_FILE=./custom.csv
      
      Options:
      COLUMN: Column name to analyze (required unless using --list-columns)
      CSV_FILE: CSV file path (optional, defaults to auto-discovery)
      COUNT: Show count of each value (true/false, default: false)
      SORT: Sort output alphabetically (true/false, default: false) 
      OUTPUT: Save results to file instead of stdout
      
      Special usage:
      go-task csv:distinct -- --list-columns    # List all available columns
      go-task csv:distinct -- --help           # Show detailed help
    vars:
      COLUMN: '{{ .COLUMN | default "" }}'
      CSV_FILE_PARAM: '{{ .CSV_FILE | default "" }}'
      ARGS: '{{ .CLI_ARGS | default "" }}'
    cmds:
      - |
        if [ "{{ .ARGS }}" != "" ]; then
          # Pass through CLI args directly (for --list-columns, --help, etc.)
          # Use dummy column name and main CSV file for --list-columns
          CSV_ARG="{{ .CSV_FILE }}"
          docker run --rm \
            --name csv-distinct-extractor \
            --workdir /app \
            --volume "$(pwd):/app" \
            --env-file .env \
            {{ .NODE_IMAGE }} \
            node utils/DistinctValueExtractor.js dummy "$CSV_ARG" {{ .ARGS }}
        elif [ "{{ .COLUMN }}" = "" ]; then
          echo "Error: COLUMN parameter is required"
          echo "Usage: go-task csv:distinct COLUMN=column_name [options]"
          echo "Or use: go-task csv:distinct -- --list-columns"
          exit 1
        else
          # Build command with parameters
          CSV_ARG="{{ if .CSV_FILE_PARAM }}{{ .CSV_FILE_PARAM }}{{ else }}{{ .CSV_FILE }}{{ end }}"
          docker run --rm \
            --name csv-distinct-extractor \
            --workdir /app \
            --volume "$(pwd):/app" \
            --env-file .env \
            {{ .NODE_IMAGE }} \
            node utils/DistinctValueExtractor.js "{{ .COLUMN }}" "$CSV_ARG" {{ .COUNT_FLAG }} {{ .SORT_FLAG }} {{ .OUTPUT_FLAG }}
        fi

  csv:extract-by-column:
    desc: "Extract CSV rows where a specified column has content (non-empty)"
    summary: |
      Extract rows from CSV files where a specified column contains content (non-empty values).
      Useful for filtering products based on specific fields like descriptions, categories, etc.
      
      Usage Examples:
      go-task csv:extract-by-column COLUMN=description
      go-task csv:extract-by-column COLUMN=short_description OUTPUT=products_with_descriptions.csv
      go-task csv:extract-by-column COLUMN=categories CSV_FILE=./custom.csv COUNT_ONLY=true
      go-task csv:extract-by-column -- --list-columns
      go-task csv:extract-by-column -- --help
      
      Options:
      COLUMN: Column name to check for content (required unless using --list-columns)
      CSV_FILE: CSV file path (optional, defaults to auto-discovery)
      OUTPUT: Save filtered results to file instead of stdout
      COUNT_ONLY: Only show count of matching rows (true/false, default: false)
      NO_HEADER: Do not include header row in output (true/false, default: false)
      
      Special usage:
      go-task csv:extract-by-column -- --list-columns  # List all available columns
      go-task csv:extract-by-column -- --help         # Show detailed help
    vars:
      COLUMN: '{{ .COLUMN | default "" }}'
      CSV_FILE_PARAM: '{{ .CSV_FILE | default "" }}'
      ARGS: '{{ .CLI_ARGS | default "" }}'
    cmds:
      - |
        if [ "{{ .ARGS }}" != "" ]; then
          # Pass through CLI args directly (for --list-columns, --help, etc.)
          # Use dummy column name and main CSV file for --list-columns
          CSV_ARG="{{ .CSV_FILE }}"
          docker run --rm \
            --name csv-column-extractor \
            --workdir /app \
            --volume "$(pwd):/app" \
            --env-file .env \
            {{ .NODE_IMAGE }} \
            node utils/ColumnContentExtractor.js dummy "$CSV_ARG" {{ .ARGS }}
        elif [ "{{ .COLUMN }}" = "" ]; then
          echo "Error: COLUMN parameter is required"
          echo "Usage: go-task csv:extract-by-column COLUMN=column_name [options]"
          echo "Or use: go-task csv:extract-by-column -- --list-columns"
          exit 1
        else
          # Build command with parameters
          CSV_ARG="{{ if .CSV_FILE_PARAM }}{{ .CSV_FILE_PARAM }}{{ else }}{{ .CSV_FILE }}{{ end }}"
          docker run --rm \
            --name csv-column-extractor \
            --workdir /app \
            --volume "$(pwd):/app" \
            --env-file .env \
            {{ .NODE_IMAGE }} \
            node utils/ColumnContentExtractor.js "{{ .COLUMN }}" "$CSV_ARG" {{ .OUTPUT_FLAG }} {{ .COUNT_ONLY_FLAG }} {{ .NO_HEADER_FLAG }}
        fi

  csv:extract-by-value:
    desc: "Extract CSV rows where a column equals a specific value"
    summary: |
      Extract rows from CSV files where a specified column equals an exact value.
      Supports case-insensitive matching and can output to files.
      
      Usage Examples:
      go-task csv:extract-by-value COLUMN=status VALUE=Enabled
      go-task csv:extract-by-value COLUMN=product_type VALUE="simple" OUTPUT=simple_products.csv
      go-task csv:extract-by-value COLUMN=manufacturer VALUE="Apple" CASE_INSENSITIVE=true COUNT_ONLY=true
      go-task csv:extract-by-value -- --list-columns
      
      Options:
      COLUMN: Column name to check (required unless using --list-columns)
      VALUE: Exact value to match (required unless using --list-columns)
      CSV_FILE: CSV file path (optional, defaults to auto-discovery)
      OUTPUT: Save filtered results to file instead of stdout
      COUNT_ONLY: Only show count of matching rows (true/false, default: false)
      NO_HEADER: Do not include header row in output (true/false, default: false)
      CASE_INSENSITIVE: Make matching case-insensitive (true/false, default: false)
    vars:
      COLUMN: '{{ .COLUMN | default "" }}'
      VALUE: '{{ .VALUE | default "" }}'
      CSV_FILE_PARAM: '{{ .CSV_FILE | default "" }}'
      ARGS: '{{ .CLI_ARGS | default "" }}'
    cmds:
      - |
        if [ "{{ .ARGS }}" != "" ]; then
          # Pass through CLI args directly (for --list-columns, --help, etc.)
          CSV_ARG="{{ .CSV_FILE }}"
          docker run --rm \
            --name csv-column-extractor \
            --workdir /app \
            --volume "$(pwd):/app" \
            --env-file .env \
            {{ .NODE_IMAGE }} \
            node utils/ColumnContentExtractor.js dummy "$CSV_ARG" {{ .ARGS }}
        elif [ "{{ .COLUMN }}" = "" ] || [ "{{ .VALUE }}" = "" ]; then
          echo "Error: Both COLUMN and VALUE parameters are required"
          echo "Usage: go-task csv:extract-by-value COLUMN=column_name VALUE=target_value [options]"
          echo "Or use: go-task csv:extract-by-value -- --list-columns"
          exit 1
        else
          # Build command with parameters
          CSV_ARG="{{ if .CSV_FILE_PARAM }}{{ .CSV_FILE_PARAM }}{{ else }}{{ .CSV_FILE }}{{ end }}"
          docker run --rm \
            --name csv-column-extractor \
            --workdir /app \
            --volume "$(pwd):/app" \
            --env-file .env \
            {{ .NODE_IMAGE }} \
            node utils/ColumnContentExtractor.js "{{ .COLUMN }}" "$CSV_ARG" --value "{{ .VALUE }}" {{ .OUTPUT_FLAG }} {{ .COUNT_ONLY_FLAG }} {{ .NO_HEADER_FLAG }} {{ .CASE_INSENSITIVE_FLAG }}
        fi

  csv:extract-by-contains:
    desc: "Extract CSV rows where a column contains a specific substring"
    summary: |
      Extract rows from CSV files where a specified column contains a substring.
      Supports case-insensitive matching and can output to files.
      
      Usage Examples:
      go-task csv:extract-by-contains COLUMN=name SUBSTRING=iPhone
      go-task csv:extract-by-contains COLUMN=description SUBSTRING="wireless" OUTPUT=wireless_products.csv
      go-task csv:extract-by-contains COLUMN=categories SUBSTRING="electronics" CASE_INSENSITIVE=true COUNT_ONLY=true
      go-task csv:extract-by-contains -- --list-columns
      
      Options:
      COLUMN: Column name to check (required unless using --list-columns)
      SUBSTRING: Substring to search for (required unless using --list-columns)
      CSV_FILE: CSV file path (optional, defaults to auto-discovery)
      OUTPUT: Save filtered results to file instead of stdout
      COUNT_ONLY: Only show count of matching rows (true/false, default: false)
      NO_HEADER: Do not include header row in output (true/false, default: false)
      CASE_INSENSITIVE: Make matching case-insensitive (true/false, default: false)
    vars:
      COLUMN: '{{ .COLUMN | default "" }}'
      SUBSTRING: '{{ .SUBSTRING | default "" }}'
      CSV_FILE_PARAM: '{{ .CSV_FILE | default "" }}'
      ARGS: '{{ .CLI_ARGS | default "" }}'
    cmds:
      - |
        if [ "{{ .ARGS }}" != "" ]; then
          # Pass through CLI args directly (for --list-columns, --help, etc.)
          CSV_ARG="{{ .CSV_FILE }}"
          docker run --rm \
            --name csv-column-extractor \
            --workdir /app \
            --volume "$(pwd):/app" \
            --env-file .env \
            {{ .NODE_IMAGE }} \
            node utils/ColumnContentExtractor.js dummy "$CSV_ARG" {{ .ARGS }}
        elif [ "{{ .COLUMN }}" = "" ] || [ "{{ .SUBSTRING }}" = "" ]; then
          echo "Error: Both COLUMN and SUBSTRING parameters are required"
          echo "Usage: go-task csv:extract-by-contains COLUMN=column_name SUBSTRING=search_text [options]"
          echo "Or use: go-task csv:extract-by-contains -- --list-columns"
          exit 1
        else
          # Build command with parameters
          CSV_ARG="{{ if .CSV_FILE_PARAM }}{{ .CSV_FILE_PARAM }}{{ else }}{{ .CSV_FILE }}{{ end }}"
          docker run --rm \
            --name csv-column-extractor \
            --workdir /app \
            --volume "$(pwd):/app" \
            --env-file .env \
            {{ .NODE_IMAGE }} \
            node utils/ColumnContentExtractor.js "{{ .COLUMN }}" "$CSV_ARG" --contains "{{ .SUBSTRING }}" {{ .OUTPUT_FLAG }} {{ .COUNT_ONLY_FLAG }} {{ .NO_HEADER_FLAG }} {{ .CASE_INSENSITIVE_FLAG }}
        fi

  csv:extract-test-products:
    vars:
      SKUS: >
        BIX.A-REM-70S
        DAA.100358
        GLK.33781
        WLK.WALGWP-SLCR2-BT
    cmds:
      - head -n1 {{ .CSV_FILE }} > {{ .CSV_FILE_TEST }}
      - for: { var: SKUS }
        cmd: python3 -c "import csv,sys,io; [csv.writer(sys.stdout).writerow(row) for file in sys.argv[1:] for row in csv.reader(open(file)) if len(row) > 0 and row[0] == '{{ .ITEM }}']" {{ .CSV_FILE }} 2>/dev/null 1>> {{ .CSV_FILE_TEST }}

  migration:customers:
    desc: "Run customer migration using Docker (without compose)"
    summary: |
      Run customer migration directly with Docker without using docker compose.
      Uses the NODE_IMAGE environment variable or defaults to node:alpine.
      
      Usage:
      go-task migration:customers
      go-task migration:customers NODE_IMAGE=node:18-alpine
      go-task migration:customers START_ROW=100 BATCH_SIZE=50
      
      Environment variables:
      NODE_IMAGE: Docker image to use (default: node:alpine)
      START_ROW: Starting row for batch processing
      BATCH_SIZE: Number of customers per batch
      All other environment variables from .env file are passed through
    cmds:
      - |
        # Create logs directory if it doesn't exist
        mkdir -p {{ .LOGS_DIR }}
        
        # Run customer migration in Docker
        docker run --rm -it \
          --name magento-shopify-customer-migration \
          --workdir /app \
          --volume "$(pwd):/app" \
          --env-file .env \
          {{ if .START_ROW }}--env "START_ROW={{ .START_ROW }}" {{ end }}\
          {{ if .BATCH_SIZE }}--env "BATCH_SIZE={{ .BATCH_SIZE }}" {{ end }}\
          {{ if .MAX_CONCURRENT }}--env "MAX_CONCURRENT={{ .MAX_CONCURRENT }}" {{ end }}\
          {{ if .DELAY_MS }}--env "DELAY_MS={{ .DELAY_MS }}" {{ end }}\
          {{ if .CUSTOMERS_CSV_PATH }}--env "CUSTOMERS_CSV_PATH={{ .CUSTOMERS_CSV_PATH }}" {{ end }}\
          {{ .NODE_IMAGE }} \
          npm run migrate-customers
  
  migration:products:
    desc: "Run product migration using Docker (without compose)"
    summary: |
      Run product migration directly with Docker without using docker compose.
      Uses the NODE_IMAGE environment variable or defaults to node:alpine.
      
      Usage:
      go-task migration:products
      go-task migration:products NODE_IMAGE=node:18-alpine
      go-task migration:products START_ROW=100 BATCH_SIZE=500
      
      Environment variables:
      NODE_IMAGE: Docker image to use (default: node:alpine)
      START_ROW: Starting row for batch processing
      BATCH_SIZE: Number of products per batch
      All other environment variables from .env file are passed through
    cmds:
      - |
        # Create logs directory if it doesn't exist
        mkdir -p {{ .LOGS_DIR }}
        
        # Run product migration in Docker
        docker run --rm -it \
          --name magento-shopify-product-migration \
          --workdir /app \
          --volume "$(pwd):/app" \
          --env-file .env \
          {{ if .START_ROW }}--env "START_ROW={{ .START_ROW }}" {{ end }}\
          {{ if .BATCH_SIZE }}--env "BATCH_SIZE={{ .BATCH_SIZE }}" {{ end }}\
          {{ if .MAX_CONCURRENT }}--env "MAX_CONCURRENT={{ .MAX_CONCURRENT }}" {{ end }}\
          {{ if .DELAY_MS }}--env "DELAY_MS={{ .DELAY_MS }}" {{ end }}\
          {{ if .CSV_PATH }}--env "CSV_PATH={{ .CSV_PATH }}" {{ end }}\
          {{ .NODE_IMAGE }} \
          npm run migrate-products
  
  migration:progress:
    cmds:
      - tail -fn1 {{ .LOGS_DIR }}/*.log | grep "Progress:"

  delete:products:
    desc: "Delete products from Shopify using CSV file with Docker"
    summary: |
      Delete products from Shopify based on SKUs listed in a CSV file.
      Uses Docker to run the deletion script in an isolated environment.
      
      Usage Examples:
      go-task delete:products                               # Delete using default settings
      go-task delete:products DELETE_CSV_PATH=./data/to_delete.csv
      go-task delete:products START_ROW=0 BATCH_SIZE=25    # Process in smaller batches
      go-task delete:products NODE_IMAGE=node:18-alpine    # Use specific Node.js version
      
      Environment Variables:
      DELETE_CSV_PATH: Path to CSV file with SKUs to delete (default: ./data/products_test.csv)
      START_ROW: Starting row for batch processing (default: 0)
      BATCH_SIZE: Number of products per batch (default: 50, smaller is safer)
      MAX_CONCURRENT: Concurrent API requests (default: 2, keep low to avoid rate limits)
      DELAY_MS: Delay between requests in milliseconds (default: 1000)
      NODE_IMAGE: Docker image to use (default: node:alpine)
      
      IMPORTANT: This will permanently delete products from your Shopify store!
      Always test with a small batch first and ensure you have backups.
    vars:
      DELETE_CSV_PATH_VAR: '{{ .DELETE_CSV_PATH | default "./data/products_test.csv" }}'
    cmds:
      - |
        # Create logs directory if it doesn't exist
        mkdir -p {{ .LOGS_DIR }}
        
        echo "⚠️  WARNING: This will permanently delete products from your Shopify store!"
        echo "CSV file: {{ .DELETE_CSV_PATH_VAR }}"
        echo "Starting deletion process..."
        
        # Run product deletion in Docker
        docker run --rm -it \
          --name shopify-product-deletion \
          --workdir /app \
          --volume "$(pwd):/app" \
          --env-file .env \
          --env "DELETE_CSV_PATH={{ .DELETE_CSV_PATH_VAR }}" \
          {{ if .START_ROW }}--env "START_ROW={{ .START_ROW }}" {{ end }}\
          {{ if .BATCH_SIZE }}--env "BATCH_SIZE={{ .BATCH_SIZE }}" {{ end }}\
          {{ if .MAX_CONCURRENT }}--env "MAX_CONCURRENT={{ .MAX_CONCURRENT }}" {{ end }}\
          {{ if .DELAY_MS }}--env "DELAY_MS={{ .DELAY_MS }}" {{ end }}\
          {{ .NODE_IMAGE }} \
          npm run delete-products
  
  delete:test-products:
    desc: "Delete the test products from products_test.csv"
    summary: |
      Convenience task to delete the test products defined in data/products_test.csv.
      This includes the SKUs: BIX.A-REM-70S, DAA.100358, GLK.33781, WLK.WALGWP-SLCR2-BT
      
      Usage:
      go-task delete:test-products                    # Delete all test products
      go-task delete:test-products BATCH_SIZE=2      # Delete in smaller batches
      go-task delete:test-products DELAY_MS=2000     # Add more delay between requests
      
      IMPORTANT: This will permanently delete the test products from your Shopify store!
    vars:
      BATCH_SIZE_DEFAULT: '{{ .BATCH_SIZE | default "4" }}'
      DELAY_MS_DEFAULT: '{{ .DELAY_MS | default "1500" }}'
    cmds:
      - |
        echo "Deleting test products from {{ .CSV_FILE_TEST }}"
        echo "This will delete products with SKUs: BIX.A-REM-70S, DAA.100358, GLK.33781, WLK.WALGWP-SLCR2-BT"
        echo "Batch size: {{ .BATCH_SIZE_DEFAULT }}, Delay: {{ .DELAY_MS_DEFAULT }}ms"
        go-task delete:products DELETE_CSV_PATH={{ .CSV_FILE_TEST }} BATCH_SIZE={{ .BATCH_SIZE_DEFAULT }} DELAY_MS={{ .DELAY_MS_DEFAULT }}

  downloader:download-csv:
    desc: "Download CSV file from authenticated web source using Docker"
    summary: |
      Download CSV files from web sources that require authentication.
      Uses Docker to run the web downloader script in an isolated environment.
      
      Usage Examples:
      go-task downloader:download-csv                           # Download using default settings
      go-task downloader:download-csv NODE_IMAGE=node:18-alpine # Use specific Node.js version
      go-task downloader:download-csv OUTPUT_DIR=./custom_data   # Custom output directory
      
      Environment Variables (set in .env file):
      LOGIN_URL: URL for authentication endpoint (required)
      DOWNLOAD_URL: URL to download CSV file from (required)
      WEB_USERNAME: Username for authentication (required)
      WEB_PASSWORD: Password for authentication (required)
      OUTPUT_DIR: Directory to save downloaded files (default: ./data)
      DOWNLOAD_FILENAME: Base filename for downloaded file (default: downloaded_file.csv)
      
      Additional Docker Options:
      NODE_IMAGE: Docker image to use (default: node:alpine)
      
      The script will:
      1. Authenticate with the provided credentials
      2. Extract and store PHPSESSID cookie
      3. Make authenticated request to download URL
      4. Save CSV file with timestamp to prevent overwrites
    cmds:
      - |
        # Create logs directory if it doesn't exist
        mkdir -p {{ .LOGS_DIR }}
        
        echo "Starting web CSV download process..."
        echo "Login URL: ${LOGIN_URL:-'Not set'}"
        echo "Download URL: ${DOWNLOAD_URL:-'Not set'}"
        echo "Output directory: ${OUTPUT_DIR:-'./data'}"
        
        # Run web downloader in Docker
        docker run --rm -it \
          --name web-csv-downloader \
          --workdir /app \
          --volume "$(pwd):/app" \
          --env-file .env \
          {{ if .OUTPUT_DIR }}--env "OUTPUT_DIR={{ .OUTPUT_DIR }}" {{ end }}\
          {{ if .DOWNLOAD_FILENAME }}--env "DOWNLOAD_FILENAME={{ .DOWNLOAD_FILENAME }}" {{ end }}\
          {{ .NODE_IMAGE }} \
          npm run web-download
